{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d44f0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7e7c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150d411",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3cc458bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "away_last10 = np.load(\"data/data_np/away_last10_X.npy\")\n",
    "home_last10 = np.load(\"data/data_np/home_last10_X.npy\")\n",
    "matchups_last3 = np.load(\"data/data_np/matchups_last3_X.npy\")\n",
    "seasonal = np.load(\"data/data_np/seasonal_stats.npy\")\n",
    "labels = np.load(\"data/data_np/last10_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "630147aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67131, 10, 117)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "away_last10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a12205f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * away_last10.shape[0])\n",
    "\n",
    "away_last10_train = away_last10[:split]\n",
    "away_last10_test = away_last10[split:]\n",
    "home_last10_train = home_last10[:split]\n",
    "home_last10_test = home_last10[split:]\n",
    "matchups_train = matchups_last3[:split]\n",
    "matchups_test = matchups_last3[split:]\n",
    "seasonal_train = seasonal[:split]\n",
    "seasonal_test = seasonal[split:]\n",
    "\n",
    "y_train = labels[:split]\n",
    "y_test = labels[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fa198f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "\n",
    "# split arrays into categorical/continuous features\n",
    "cat_ind = [1, 3, 4, 61, 62]\n",
    "away_l10_train_cat = away_last10_train[:, :, cat_ind]\n",
    "away_l10_train_con = np.delete(away_last10_train, cat_ind, axis=2)\n",
    "\n",
    "away_l10_test_cat = away_last10_test[:, :, cat_ind]\n",
    "away_l10_test_con = np.delete(away_last10_test, cat_ind, axis=2)\n",
    "\n",
    "home_l10_train_cat = home_last10_train[:, :, cat_ind]\n",
    "home_l10_train_con = np.delete(home_last10_train, cat_ind, axis=2)\n",
    "\n",
    "home_l10_test_cat = home_last10_test[:, :, cat_ind]\n",
    "home_l10_test_con = np.delete(home_last10_test, cat_ind, axis=2)\n",
    "\n",
    "match_train_cat = matchups_train[:, :, cat_ind]\n",
    "match_train_con = np.delete(matchups_train, cat_ind, axis=2)\n",
    "\n",
    "match_test_cat = matchups_test[:, :, cat_ind]\n",
    "match_test_con = np.delete(matchups_test, cat_ind, axis=2)\n",
    "\n",
    "cat_ind = [0, 1, 2]\n",
    "season_train_cat = seasonal_train[:, cat_ind]\n",
    "season_train_con = np.delete(seasonal_train, cat_ind, axis=1)\n",
    "\n",
    "season_test_cat = seasonal_test[:, cat_ind]\n",
    "season_test_con = np.delete(seasonal_test, cat_ind, axis=1)\n",
    "\n",
    "# Flatten arrays, fit/apply standard scalar to continuous features\n",
    "n1 = away_l10_train_con.shape[0]\n",
    "s1 = away_l10_train_con.shape[1]\n",
    "d1 = away_l10_train_con.shape[2]\n",
    "\n",
    "n2 = away_l10_test_con.shape[0]\n",
    "s2 = away_l10_test_con.shape[1]\n",
    "d2 = away_l10_test_con.shape[2]\n",
    "\n",
    "away10_scaler = StandardScaler()\n",
    "\n",
    "away_l10_train_con = away_l10_train_con.reshape((n1 * s1, d1))\n",
    "away_l10_train_con = away10_scaler.fit_transform(away_l10_train_con)\n",
    "away_l10_train_con = away_l10_train_con.reshape((n1, s1, d1))\n",
    "\n",
    "away_l10_test_con = away_l10_test_con.reshape((n2 * s2, d2))\n",
    "away_l10_test_con = away10_scaler.transform(away_l10_test_con)\n",
    "away_l10_test_con = away_l10_test_con.reshape((n2, s2, d2))\n",
    "\n",
    "home10_scaler = StandardScaler()\n",
    "\n",
    "home_l10_train_con = home_l10_train_con.reshape((n1 * s1, d1))\n",
    "home_l10_train_con = home10_scaler.fit_transform(home_l10_train_con)\n",
    "home_l10_train_con = home_l10_train_con.reshape((n1, s1, d1))\n",
    "\n",
    "home_l10_test_con = home_l10_test_con.reshape((n2 * s2, d2))\n",
    "home_l10_test_con = home10_scaler.transform(home_l10_test_con)\n",
    "home_l10_test_con = home_l10_test_con.reshape((n2, s2, d2))\n",
    "\n",
    "n1 = match_train_con.shape[0]\n",
    "s1 = match_train_con.shape[1]\n",
    "d1 = match_train_con.shape[2]\n",
    "\n",
    "n2 = match_test_con.shape[0]\n",
    "s2 = match_test_con.shape[1]\n",
    "d2 = match_test_con.shape[2]\n",
    "\n",
    "matchup_scaler = StandardScaler()\n",
    "\n",
    "match_train_con = match_train_con.reshape((n1 * s1, d1))\n",
    "match_train_con = matchup_scaler.fit_transform(match_train_con)\n",
    "match_train_con = match_train_con.reshape((n1, s1, d1))\n",
    "\n",
    "match_test_con = match_test_con.reshape((n2 * s2, d2))\n",
    "match_test_con = matchup_scaler.fit_transform(match_test_con)\n",
    "match_test_con = match_test_con.reshape((n2, s2, d2))\n",
    "\n",
    "season_scaler = StandardScaler()\n",
    "\n",
    "season_train_con = season_scaler.fit_transform(season_train_con)\n",
    "season_test_con = season_scaler.transform(season_test_con)\n",
    "\n",
    "# Re-concatenate the categorical/continuous features\n",
    "away_last10_train = np.concatenate((away_l10_train_con, away_l10_train_cat), axis=2)\n",
    "away_last10_test = np.concatenate((away_l10_test_con, away_l10_test_cat), axis=2)\n",
    "home_last10_train = np.concatenate((home_l10_train_con, home_l10_train_cat), axis=2)\n",
    "home_last10_test = np.concatenate((away_l10_test_con, away_l10_test_cat), axis=2)\n",
    "\n",
    "matchups_train = np.concatenate((match_train_con, match_train_cat), axis=2)\n",
    "matchups_test = np.concatenate((match_test_con, match_test_cat), axis=2)\n",
    "\n",
    "seasonal_train = np.concatenate((season_train_con, season_train_cat), axis=1)\n",
    "seasonal_test = np.concatenate((season_test_con, season_test_cat), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e27087df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train/val\n",
    "val_split = int(0.8 * away_last10_train.shape[0])\n",
    "\n",
    "away_last10_val = away_last10_train[val_split:]\n",
    "away_last10_train = away_last10_train[:val_split]\n",
    "\n",
    "home_last10_val = home_last10_train[val_split:]\n",
    "home_last10_train = home_last10_train[:val_split]\n",
    "\n",
    "matchups_val = matchups_train[val_split:]\n",
    "matchups_train = matchups_train[:val_split]\n",
    "\n",
    "seasonal_val = seasonal_train[val_split:]\n",
    "seasonal_train = seasonal_train[:val_split]\n",
    "\n",
    "y_val = y_train[val_split:]\n",
    "y_train = y_train[:val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d80f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape: (42963, 10, 117) test_shape (13427, 10, 117)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_shape:\", away_last10_train.shape, \"test_shape\", away_last10_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0e04c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Datasets\n",
    "np.save(\"data/data_np/away_last10_train.npy\", away_last10_train)\n",
    "np.save(\"data/data_np/away_last10_val.npy\", away_last10_val)\n",
    "np.save(\"data/data_np/away_last10_test.npy\", away_last10_test)\n",
    "\n",
    "np.save(\"data/data_np/home_last10_train.npy\", home_last10_train)\n",
    "np.save(\"data/data_np/home_last10_val.npy\", home_last10_val)\n",
    "np.save(\"data/data_np/home_last10_test.npy\", home_last10_test)\n",
    "\n",
    "np.save(\"data/data_np/matchups_train.npy\", matchups_train)\n",
    "np.save(\"data/data_np/matchups_val.npy\", matchups_val)\n",
    "np.save(\"data/data_np/matchups_test.npy\", matchups_test)\n",
    "\n",
    "np.save(\"data/data_np/seasonal_train.npy\", seasonal_train)\n",
    "np.save(\"data/data_np/seasonal_val.npy\", seasonal_val)\n",
    "np.save(\"data/data_np/seasonal_test.npy\", seasonal_test)\n",
    "\n",
    "np.save(\"data/data_np/y_train.npy\", y_train)\n",
    "np.save(\"data/data_np/y_val.npy\", y_val)\n",
    "np.save(\"data/data_np/y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d27f6742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13427, 10, 117)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "away_last10_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aec506",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c87b7ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader batches: 672, Test DataLoader batches: 210\n"
     ]
    }
   ],
   "source": [
    "away_last10_train = np.load(\"data/data_np/away_last10_train.npy\")\n",
    "away_last10_val = np.load(\"data/data_np/away_last10_val.npy\")\n",
    "away_last10_test = np.load(\"data/data_np/away_last10_test.npy\")\n",
    "\n",
    "home_last10_train = np.load(\"data/data_np/home_last10_train.npy\")\n",
    "home_last10_val = np.load(\"data/data_np/home_last10_val.npy\")\n",
    "home_last10_test = np.load(\"data/data_np/home_last10_test.npy\")\n",
    "\n",
    "matchups_train = np.load(\"data/data_np/matchups_train.npy\")\n",
    "matchups_val = np.load(\"data/data_np/matchups_val.npy\")\n",
    "matchups_test = np.load(\"data/data_np/matchups_test.npy\")\n",
    "\n",
    "seasonal_train = np.load(\"data/data_np/seasonal_train.npy\")\n",
    "seasonal_val = np.load(\"data/data_np/seasonal_val.npy\")\n",
    "seasonal_test = np.load(\"data/data_np/seasonal_test.npy\")\n",
    "\n",
    "y_train = np.load(\"data/data_np/y_train.npy\")\n",
    "y_val = np.load(\"data/data_np/y_val.npy\")\n",
    "y_test = np.load(\"data/data_np/y_test.npy\")\n",
    "\n",
    "# Convert to torch tensors\n",
    "X1_train_tensor = torch.tensor(away_last10_train, dtype=torch.float32).to(device)\n",
    "X1_val_tensor = torch.tensor(away_last10_val, dtype=torch.float32).to(device)\n",
    "X1_test_tensor = torch.tensor(away_last10_test, dtype=torch.float32).to(device)\n",
    "\n",
    "X2_train_tensor = torch.tensor(home_last10_train, dtype=torch.float32).to(device)\n",
    "X2_val_tensor = torch.tensor(home_last10_val, dtype=torch.float32).to(device)\n",
    "X2_test_tensor = torch.tensor(home_last10_test, dtype=torch.float32).to(device)\n",
    "\n",
    "X3_train_tensor = torch.tensor(matchups_train, dtype=torch.float32).to(device)\n",
    "X3_val_tensor = torch.tensor(matchups_val, dtype=torch.float32).to(device)\n",
    "X3_test_tensor = torch.tensor(matchups_test, dtype=torch.float32).to(device)\n",
    "\n",
    "X4_train_tensor = torch.tensor(seasonal_train, dtype=torch.float32).to(device)\n",
    "X4_val_tensor = torch.tensor(seasonal_val, dtype=torch.float32).to(device)\n",
    "X4_test_tensor = torch.tensor(seasonal_test, dtype=torch.float32).to(device)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Make dataset\n",
    "class MultiInputDataset(Dataset):\n",
    "\tdef __init__(self, X1, X2, X3, X4, y):\n",
    "\t\tself.X1 = X1\n",
    "\t\tself.X2 = X2\n",
    "\t\tself.X3 = X3\n",
    "\t\tself.X4 = X4\n",
    "\t\tself.y = y\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X1)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.X1[idx], self.X2[idx], self.X3[idx], self.X4[idx], self.y[idx]\n",
    "\t\n",
    "train_dataset = MultiInputDataset(X1_train_tensor, X2_train_tensor, X3_train_tensor, X4_train_tensor, y_train_tensor)\n",
    "val_dataset = MultiInputDataset(X1_val_tensor, X2_val_tensor, X3_val_tensor, X4_val_tensor, y_val_tensor)\n",
    "test_dataset = MultiInputDataset(X1_test_tensor, X2_test_tensor, X3_test_tensor, X4_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train DataLoader batches: {len(train_loader)}, Test DataLoader batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b92ba8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def sinusoidal_encode(self, sequence_len, d_model):\n",
    "        pos = np.arange(sequence_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pe = np.zeros((sequence_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(pos * div_term)\n",
    "        pe[:, 1::2] = np.cos(pos * div_term)\n",
    "\n",
    "        # return torch.tensor(pe, dtype=torch.float32).to(device)\n",
    "        pe_tensor = torch.tensor(pe, dtype=torch.float32).to(device)\n",
    "        return pe_tensor.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        sequence_len = x.shape[1]\n",
    "        encode = self.sinusoidal_encode(sequence_len, self.d_model)\n",
    "\n",
    "        x = self.input(x)\n",
    "        x = x + encode\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        # encoder_out = encoder_out[:, -1, :]\n",
    "        return x\n",
    "\n",
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, feat_dim1, feat_dim2, feat_dim3, d_model, num_heads, num_layers, dropout):\n",
    "        super(TransformerPredictor, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.encoder1 = TimeSeriesTransformer(feat_dim1, d_model, num_heads, num_layers, dropout)\n",
    "        self.encoder2 = TimeSeriesTransformer(feat_dim1, d_model, num_heads, num_layers, dropout)\n",
    "        self.encoder3 = TimeSeriesTransformer(feat_dim2, d_model, num_heads, num_layers, dropout)\n",
    "\n",
    "        self.seasonal_layers = nn.Sequential(\n",
    "            nn.Linear(feat_dim3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, d_model)\n",
    "        )\n",
    "\n",
    "        self.combined_layers = nn.Sequential(\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.encoder1(x1)\n",
    "        x2 = self.encoder2(x2)\n",
    "        x3 = self.encoder3(x3)\n",
    "        x4 = self.seasonal_layers(x4)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=-1)\n",
    "        x = self.combined_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23af1868",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X1_batch, X2_batch, X3_batch, X4_batch)\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input_dim = X_train_tensor.shape[2]\n",
    "# hidden_dim = 256\n",
    "# num_layers = 2\n",
    "# output_dim = 1\n",
    "# dropout = 0.5\n",
    "\n",
    "# model = LSTMPredictor(input_dim, hidden_dim, num_layers, output_dim, dropout).to(device)\n",
    "feat_dim1 = X1_train_tensor.shape[2]\n",
    "feat_dim2 = X3_train_tensor.shape[2]\n",
    "feat_dim3 = X4_train_tensor.shape[1]\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "model = TransformerPredictor(feat_dim1, feat_dim2, feat_dim3, d_model, num_heads, num_layers, dropout)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for X1_batch, X2_batch, X3_batch, X4_batch, y_batch in train_loader:\n",
    "        X1_batch = X1_batch.to(device)\n",
    "        X2_batch = X2_batch.to(device)\n",
    "        X3_batch = X3_batch.to(device)\n",
    "        X4_batch = X4_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X1_batch, X2_batch, X3_batch, X4_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X1_val_batch, X2_val_batch, X3_val_batch, X4_val_batch, y_val_batch in val_loader:\n",
    "            val_outputs = model(X1_val_batch, X2_val_batch, X3_val_batch, X4_val_batch)\n",
    "            val_loss_batch = criterion(val_outputs, y_val_batch)\n",
    "            epoch_val_loss += val_loss_batch.item()\n",
    "    epoch_val_loss /= len(val_loader)\n",
    "    val_loss.append(epoch_val_loss)\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    train_loss.append(epoch_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.9f}, Val Loss: {epoch_val_loss:.9f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
